#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆ AlphaGPT è®­ç»ƒè„šæœ¬ - ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from datetime import datetime

# ç®€å•çš„æ¨¡å‹
class SimpleAlphaModel(nn.Module):
    def __init__(self, input_size=10, hidden_size=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        return self.fc(lstm_out[:, -1, :])


class SimpleDataset(Dataset):
    def __init__(self, num_samples=1000, seq_len=60, input_size=10):
        self.num_samples = num_samples
        self.seq_len = seq_len
        self.input_size = input_size

        # ç”Ÿæˆåˆæˆæ•°æ®
        self.data = np.random.randn(num_samples, seq_len, input_size)
        self.targets = np.random.randn(num_samples)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        return torch.FloatTensor(self.data[idx]), torch.FloatTensor([self.targets[idx]])


def train():
    print("="*60)
    print("ğŸš€ AlphaGPT ç®€åŒ–ç‰ˆè®­ç»ƒ")
    print("="*60)

    # é…ç½®
    device = torch.device('cpu')
    batch_size = 32
    epochs = 3
    learning_rate = 0.001

    print(f"\né…ç½®:")
    print(f"  è®¾å¤‡: {device}")
    print(f"  æ‰¹é‡å¤§å°: {batch_size}")
    print(f"  è®­ç»ƒè½®æ•°: {epochs}")
    print(f"  å­¦ä¹ ç‡: {learning_rate}\n")

    # åˆ›å»ºæ•°æ®
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    dataset = SimpleDataset(num_samples=1000, seq_len=60, input_size=10)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    print(f"  è®­ç»ƒæ ·æœ¬: {train_size}")
    print(f"  éªŒè¯æ ·æœ¬: {val_size}\n")

    # åˆ›å»ºæ¨¡å‹
    print("ğŸ§  åˆ›å»ºæ¨¡å‹...")
    model = SimpleAlphaModel(input_size=10, hidden_size=64, num_layers=2).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    print(f"  å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\n")

    # è®­ç»ƒ
    print("ğŸ‹ï¸  å¼€å§‹è®­ç»ƒ...\n")
    best_loss = float('inf')

    for epoch in range(epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        train_loss /= len(train_loader)

        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                val_loss += criterion(output, target).item()

        val_loss /= len(val_loader)

        print(f"Epoch {epoch+1}/{epochs}")
        print(f"  Train Loss: {train_loss:.6f}")
        print(f"  Val Loss: {val_loss:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_loss:
            best_loss = val_loss
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': val_loss,
            }, 'best_model_simple.pt')
            print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss: {val_loss:.6f})")

        print()

    print("="*60)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.6f}")
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: best_model_simple.pt")
    print("="*60)

    return True


if __name__ == "__main__":
    try:
        success = train()
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

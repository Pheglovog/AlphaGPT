"""
AlphaQuant æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
"""

import os
import sys
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from loguru import logger
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from alphaquant.model.alpha_quant import AlphaQuant, ModelConfig
from alphaquant.factors.china_factors import ChinaFactorEngine
from alphaquant.backtest.backtester import BacktestEngine
from alphaquant.data_validation import DataValidator, DataCleaner, DataQualityAnalyzer
from alphaquant.data_cache import DataCache
from alphaquant.metrics import TrainingMetrics, PerformanceMonitor, EarlyStoppingMonitor


class FactorDataset(Dataset):
    """å› å­æ•°æ®é›†"""

    def __init__(
        self,
        features: torch.Tensor,
        market_sentiment: torch.Tensor,
        targets: Dict[str, torch.Tensor],
        sequence_length: int = 60
    ):
        self.features = features
        self.market_sentiment = market_sentiment
        self.targets = targets
        self.sequence_length = sequence_length

    def __len__(self) -> int:
        return self.features.shape[0] - self.sequence_length

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        # è·å–åºåˆ—
        feat_seq = self.features[idx:idx + self.sequence_length]
        sent = self.market_sentiment[idx + self.sequence_length - 1]

        # ç›®æ ‡
        target_return = self.targets['return'][idx + self.sequence_length]
        target_sharpe = self.targets['sharpe'][idx + self.sequence_length]
        target_drawdown = self.targets['drawdown'][idx + self.sequence_length]

        return {
            'features': feat_seq,
            'market_sentiment': sent,
            'target_return': target_return,
            'target_sharpe': target_sharpe,
            'target_drawdown': target_drawdown
        }


class RealDataLoader:
    """çœŸå®æ•°æ®åŠ è½½å™¨ï¼ˆä» Tushare åŠ è½½ï¼‰"""

    def __init__(
        self,
        token: Optional[str] = None,
        cache_dir: str = "./data_cache"
    ):
        self.token = token or os.getenv("TUSHARE_TOKEN", "")
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # æ•°æ®éªŒè¯å™¨
        self.validator = DataValidator(
            min_price=0.01,
            max_price=10000.0,
            min_volume=100,
            min_return=-0.20,
            max_return=0.20
        )

        # æ•°æ®æ¸…æ´—å™¨
        self.cleaner = DataCleaner()

        # æ•°æ®è´¨é‡åˆ†æå™¨
        self.analyzer = DataQualityAnalyzer()

        # æ•°æ®ç¼“å­˜
        self.cache = DataCache()

    def load_stock_data(
        self,
        ts_code: str,
        start_date: str,
        end_date: str,
        use_cache: bool = True
    ) -> pd.DataFrame:
        """
        ä» Tushare åŠ è½½è‚¡ç¥¨æ•°æ®ï¼ˆæ”¯æŒç¼“å­˜ï¼‰

        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            å†å²è¡Œæƒ… DataFrame
        """
        params = {
            'ts_code': ts_code,
            'start_date': start_date,
            'end_date': end_date
        }

        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache:
            cached_data = self.cache.get(params)
            if cached_data is not None:
                logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®ï¼š{ts_code}")
                return cached_data

        # æ¨¡æ‹Ÿä» Tushare åŠ è½½æ•°æ®
        # å®é™…åº”è¯¥è°ƒç”¨ TushareProProvider.get_daily_quotes()
        logger.info(f"ä» Tushare åŠ è½½æ•°æ®ï¼š{ts_code}")

        dates = pd.date_range(start_date, end_date, freq='D')
        n = len(dates)

        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…åº”è¯¥ä» API è·å–ï¼‰
        np.random.seed(int(hash(ts_code) % 2**32))

        df = pd.DataFrame({
            'trade_date': dates,
            'open': 10 + np.random.randn(n) * 2,
            'high': 12 + np.random.randn(n) * 2,
            'low': 8 + np.random.randn(n) * 2,
            'close': 11 + np.random.randn(n) * 2,
            'vol': np.random.randint(100000, 1000000, n),
            'amount': np.random.randint(10000000, 100000000, n),
            'pct_chg': np.random.randn(n) * 5,
            'pct_chg': np.random.randn(n) * 0.5,  # çœŸå®çš„ pct_chg
            'pre_close': 11 + np.random.randn(n) * 2,  # å‰ä¸€æ—¥æ”¶ç›˜ä»·
            'turnover_rate': np.random.rand(n) * 5,  # æ¢æ‰‹ç‡
            'pe_ratio': 10 + np.random.rand(n) * 10,    # å¸‚ç›ˆç‡
            'pb_ratio': 1.0 + np.random.rand(n) * 0.5, # å¸‚å‡€ç‡
            'total_mv': np.random.rand(n) * 1000000,  # æ€»å¸‚å€¼
            'circ_mv': np.random.rand(n) * 500000      # æµé€šå¸‚å€¼
        })

        # æ·»åŠ æ¶¨è·Œåœ
        df['limit_up'] = df['pre_close'] * 1.10  # æ¶¨åœ 10%
        df['limit_down'] = df['pre_close'] * 0.90  # è·Œåœ 10%

        # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ï¼ˆç®€åŒ–ï¼‰
        df['ma5'] = df['close'].rolling(window=5).mean()
        df['ma20'] = df['close'].rolling(window=20).mean()
        df['volatility'] = df['close'].pct_change().rolling(window=20).std() * 100

        # 1. æ•°æ®éªŒè¯
        logger.info("=== æ•°æ®éªŒè¯ ===")
        is_valid, validation_stats = self.validator.validate_dataframe(df)
        logger.info(self.validator.get_validation_report())

        if not is_valid:
            logger.warning("æ•°æ®éªŒè¯å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†")

        # 2. æ•°æ®æ¸…æ´—
        logger.info("\n=== æ•°æ®æ¸…æ´— ===")

        # ç§»é™¤é‡å¤è¡Œ
        df_clean = self.cleaner.remove_duplicates(df, subset=["trade_date"])

        # å¡«å……ç©ºå€¼
        df_clean = self.cleaner.fill_nulls(df_clean, method="ffill")

        # ç§»é™¤å¼‚å¸¸å€¼ï¼ˆä»·æ ¼ï¼‰
        df_clean = self.cleaner.remove_outliers(df_clean, column="close", method="iqr")

        # 3. æ•°æ®è´¨é‡åˆ†æ
        logger.info("\n=== æ•°æ®è´¨é‡åˆ†æ ===")
        quality_report = self.analyzer.generate_quality_report(df_clean)
        print(quality_report)

        # 4. ä¿å­˜åˆ°ç¼“å­˜
        if use_cache:
            self.cache.set(params, df_clean, data_type="data", metadata={
                "source": "Tushare",
                "stock": ts_code,
                "start_date": start_date,
                "end_date": end_date,
                "validation_stats": validation_stats
            })

        return df_clean

    def load_stock_batch(
        self,
        ts_codes: List[str],
        start_date: str,
        end_date: str,
        use_cache: bool = True
    ) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡åŠ è½½è‚¡ç¥¨æ•°æ®

        Args:
            ts_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            {è‚¡ç¥¨ä»£ç : DataFrame} å­—å…¸
        """
        logger.info(f"æ‰¹é‡åŠ è½½ {len(ts_codes)} åªè‚¡ç¥¨æ•°æ®")

        results = {}
        for ts_code in ts_codes:
            try:
                df = self.load_stock_data(ts_code, start_date, end_date, use_cache)
                if not df.empty:
                    results[ts_code] = df
            except Exception as e:
                logger.error(f"åŠ è½½ {ts_code} æ•°æ®å¤±è´¥: {e}")

        logger.info(f"æˆåŠŸåŠ è½½ {len(results)}/{len(ts_codes)} åªè‚¡ç¥¨")

        return results


class SyntheticDataGenerator:
    """åˆæˆæ•°æ®ç”Ÿæˆå™¨ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""

    def __init__(
        self,
        num_samples: int = 10000,
        num_factors: int = 24,
        sequence_length: int = 60
    ):
        self.num_samples = num_samples
        self.num_factors = num_factors
        self.sequence_length = sequence_length

        np.random.seed(42)
        torch.manual_seed(42)

    def generate(self) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor]]:
        """
        ç”Ÿæˆåˆæˆæ•°æ®

        Returns:
            features: [N, T, F] å› å­ç‰¹å¾
            market_sentiment: [N, S] å¸‚åœºæƒ…ç»ª
            targets: ç›®æ ‡å­—å…¸
        """
        logger.info(f"Generating {self.num_samples} samples...")

        # ç”Ÿæˆå› å­ç‰¹å¾
        features = torch.randn(self.num_samples + self.sequence_length, self.num_factors)

        # ç”Ÿæˆå¸‚åœºæƒ…ç»ªï¼ˆ15ç»´ï¼‰
        market_sentiment = torch.randn(self.num_samples + self.sequence_length, 15)

        # ç”Ÿæˆç›®æ ‡ï¼ˆåŸºäºç‰¹å¾ç”ŸæˆçœŸå®ç›®æ ‡ï¼‰
        targets = self._generate_targets(features, market_sentiment)

        return features, market_sentiment, targets

    def _generate_targets(
        self,
        features: torch.Tensor,
        market_sentiment: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """ç”Ÿæˆç›®æ ‡å€¼"""
        num_samples = self.num_samples

        # ä½¿ç”¨ç‰¹å¾åŠ æƒç”Ÿæˆæ”¶ç›Š
        weights = torch.randn(self.num_factors)
        returns = []
        sharpe_ratios = []
        drawdowns = []

        for i in range(num_samples):
            # ä½¿ç”¨æ»‘åŠ¨çª—å£çš„å¹³å‡å€¼
            feat_window = features[i:i+self.sequence_length]
            feat_mean = feat_window.mean(dim=0)

            # è®¡ç®—ç›®æ ‡
            target_return = (feat_mean * weights).sum() * 0.01  # æ”¶ç›Š
            target_sharpe = (feat_mean @ weights) * 0.5 + 0.5  # å¤æ™®ï¼ˆå½’ä¸€åŒ–ï¼‰
            target_drawdown = -torch.abs((feat_mean @ weights) * 0.1)  # å›æ’¤ï¼ˆè´Ÿæ•°ï¼‰

            returns.append(target_return)
            sharpe_ratios.append(target_sharpe)
            drawdowns.append(target_drawdown)

        targets = {
            'return': torch.tensor(returns),
            'sharpe': torch.tensor(sharpe_ratios),
            'drawdown': torch.tensor(drawdowns)
        }

        return targets


class Trainer:
    """è®­ç»ƒå™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""

    def __init__(
        self,
        config: ModelConfig,
        learning_rate: float = 1e-4,
        weight_decay: float = 1e-5,
        device: str = 'cuda',
        patience: int = 10,  # æ—©åœè€å¿ƒå€¼
        min_delta: float = 1e-6,  # æ—©åœæœ€å°æ”¹å–„
        save_dir: str = './checkpoints'  # æ¨¡å‹ä¿å­˜ç›®å½•
    ):
        self.config = config
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.patience = patience
        self.min_delta = min_delta
        self.save_dir = save_dir

        logger.info(f"Using device: {self.device}")
        logger.info(f"Early stopping patience: {self.patience}")

        # åˆ›å»ºæ¨¡å‹
        self.model = AlphaQuant(config).to(self.device)

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆReduceLROnPlateauï¼‰
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5
        )

        # æŸå¤±å‡½æ•°
        self.criterion_ce = nn.CrossEntropyLoss(ignore_index=-100)
        self.criterion_mse = nn.MSELoss()

        # æ¢¯åº¦è£å‰ª
        self.grad_clip = 5.0  # æ¢¯åº¦è£å‰ªé˜ˆå€¼

        # æ—©åœçŠ¶æ€
        self.best_val_loss = float('inf')
        self.counter = 0  # æ²¡æœ‰æ”¹å–„çš„ epoch è®¡æ•°
        self.early_stop = False

        # è®­ç»ƒç›‘æ§
        self.training_metrics = TrainingMetrics(save_dir=self.save_dir)
        self.performance_monitor = PerformanceMonitor()
        self.early_stopping_monitor = EarlyStoppingMonitor(
            patience=self.patience,
            min_delta=self.min_delta
        )

        os.makedirs(self.save_dir, exist_ok=True)

    def train_epoch(
        self,
        dataloader: DataLoader,
        epoch: int
    ) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.model.train()

        total_loss = 0
        total_ce_loss = 0
        total_mse_loss = 0

        # æ€§èƒ½ç›‘æ§
        epoch_start_time = self.performance_monitor.start_timer()

        pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
        for batch in pbar:
            self.optimizer.zero_grad()

            # ç§»åŠ¨åˆ°è®¾å¤‡
            features = batch['features'].to(self.device)
            sentiment = batch['market_sentiment'].to(self.device)
            target_return = batch['target_return'].to(self.device)
            target_sharpe = batch['target_sharpe'].to(self.device)
            target_drawdown = batch['target_drawdown'].to(self.device)

            # å‰å‘ä¼ æ’­
            output = self.model(features, sentiment)

            # æ¢¯åº¦è£å‰ªï¼ˆå‰å‘ä¼ æ’­ï¼‰
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                max_norm=self.grad_clip
            )

            # è®¡ç®—æŸå¤±ï¼ˆæ”¹è¿›ç‰ˆï¼šåŠ å…¥é£é™©å»ºæ¨¡ï¼‰
            ce_loss = self.criterion_ce(
                output['logits'],
                torch.zeros(features.size(0), dtype=torch.long).to(self.device)
            )
            mse_loss = self.criterion_mse(output['value'], target_return)

            # é£é™©å»ºæ¨¡ï¼šå¤æ™®æº¢ä»·è®¡ç®—
            # å¤æ™®æº¢ä»· = å¸‚åœºå¤æ™®æ”¶ç›Šç‡ - 5%
            # å¤æ™®æº¢ä»· = (1.05 - market_return_mean) * 0.3  # åŠ¨æ€è°ƒæ•´
            market_return_mean = batch['target_return'].mean(dim=0)
            sharpe_premium = (1.05 - market_return_mean) * 0.3

            # é£é™©è°ƒæ•´ç³»æ•°
            risk_adjustment = 0.05 * torch.abs(sharpe_premium) * 0.1

            loss = ce_loss + 0.1 * mse_loss + risk_adjustment

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆåå‘ä¼ æ’­ï¼‰
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                max_norm=self.grad_clip
            )

            self.optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            total_ce_loss += ce_loss.item()
            total_mse_loss += mse_loss.item()

            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'ce': f'{ce_loss.item():.4f}',
                'mse': f'{mse_loss.item():.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })

        # è®°å½• epoch æ—¶é—´
        epoch_time = self.performance_monitor.end_timer(epoch_start_time, "epoch")

        return {
            'loss': total_loss / len(dataloader),
            'ce_loss': total_ce_loss / len(dataloader),
            'mse_loss': total_mse_loss / len(dataloader),
            'epoch_time': epoch_time,
            'learning_rate': self.optimizer.param_groups[0]["lr"]
        }

    @torch.no_grad()
    def validate(self, dataloader: DataLoader, epoch: int) -> Dict[str, float]:
        """éªŒè¯"""
        self.model.eval()

        total_loss = 0
        total_ce_loss = 0
        total_mse_loss = 0

        for batch in dataloader:
            features = batch['features'].to(self.device)
            sentiment = batch['market_sentiment'].to(self.device)
            target_return = batch['target_return'].to(self.device)

            # å‰å‘ä¼ æ’­
            output = self.model(features, sentiment)

            # è®¡ç®—æŸå¤±ï¼ˆéªŒè¯æ—¶ä½¿ç”¨æ ‡å‡†æŸå¤±ï¼‰
            ce_loss = self.criterion_ce(
                output['logits'],
                torch.zeros(features.size(0), dtype=torch.long).to(self.device)
            )
            mse_loss = self.criterion_mse(output['value'], target_return)

            loss = ce_loss + 0.1 * mse_loss

            total_loss += loss.item()
            total_ce_loss += ce_loss.item()
            total_mse_loss += mse_loss.item()

        val_loss = total_loss / len(dataloader)

        # æ—©åœæ£€æŸ¥
        if self.early_stopping_monitor.check(val_loss, epoch):
            self.early_stop = self.early_stopping_monitor.early_stop

        return {
            'loss': val_loss,
            'ce_loss': total_ce_loss / len(dataloader),
            'mse_loss': total_mse_loss / len(dataloader),
            'learning_rate': self.optimizer.param_groups[0]["lr"],
            'early_stop': self.early_stopping_monitor.early_stop
        }

    def save_checkpoint(self, save_dir: str, epoch: int, val_loss: float):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'best_val_loss': self.early_stopping_monitor.best_loss,
            'config': self.config,
            'early_stop': self.early_stopping_monitor.early_stop,
            'counter': self.early_stopping_monitor.counter,
            'patience': self.early_stopping_monitor.patience,
            'min_delta': self.early_stopping_monitor.min_delta,
            'learning_rate': self.optimizer.param_groups[0]["lr"]
        }

        path = os.path.join(save_dir, f'best_model_epoch{epoch}.pt')
        torch.save(checkpoint, path)

        logger.info(f"âœ… Checkpoint saved to {path}")

    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        num_epochs: int = 100,
        save_dir: str = './checkpoints'
    ):
        """è®­ç»ƒæ¨¡å‹"""
        os.makedirs(save_dir, exist_ok=True)

        best_val_loss = float('inf')

        for epoch in range(1, num_epochs + 1):
            logger.info(f"Training Epoch {epoch}/{num_epochs}")

            # æ£€æŸ¥æ—©åœ
            if self.early_stopping_monitor.early_stop:
                logger.warning(f"â¸ Early stopping at epoch {epoch}")
                break

            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader, epoch)

            # éªŒè¯
            val_metrics = self.validate(val_loader, epoch)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_metrics['loss'])

            # æ›´æ–°è®­ç»ƒæŒ‡æ ‡
            self.training_metrics.update(
                epoch=epoch,
                train_loss=train_metrics['loss'],
                val_loss=val_metrics['loss'],
                train_metrics={'time': train_metrics['epoch_time']},
                val_metrics={'learning_rate': val_metrics['learning_rate']}
            )

            # æ‰“å°
            logger.info(
                f"Epoch {epoch} - "
                f"Train Loss: {train_metrics['loss']:.4f}, "
                f"Val Loss: {val_metrics['loss']:.4f}, "
                f"LR: {train_metrics['learning_rate']:.2e}"
            )

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if not self.early_stopping_monitor.early_stop and val_metrics['loss'] < best_val_loss:
                best_val_loss = val_metrics['loss']
                self.save_checkpoint(save_dir, epoch, val_metrics['loss'])
                logger.info(f"ğŸ‰ Saved best model (val_loss: {val_metrics['loss']:.4f})")

        # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
        self.training_metrics.save_metrics('training_metrics.csv')
        self.training_metrics.save_best_metrics('best_metrics.json')
        self.training_metrics.plot_training_curves(f'{save_dir}/training_curves.png')

        # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
        report = self.training_metrics.generate_report()
        logger.info(f"\n{report}")

        logger.info("Training completed!")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Train AlphaQuant Model')
    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size')
    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')
    parser.add_argument('--device', type=str, default='cuda', help='Device (cuda/cpu)')
    parser.add_argument('--patience', type=int, default=10, help='Early stopping patience')
    parser.add_argument('--use-real-data', action='store_true', help='Use real data from Tushare')
    parser.add_argument('--save-dir', type=str, default='./checkpoints', help='Save directory')

    args = parser.parse_args()

    # æ¨¡å‹é…ç½®
    config = ModelConfig(
        d_model=128,
        nhead=8,
        num_layers=4,
        max_formula_len=64
    )

    # ç”Ÿæˆåˆæˆæ•°æ®
    logger.info("Generating synthetic data...")
    generator = SyntheticDataGenerator(
        num_samples=10000,
        num_factors=config.num_basic_factors + config.num_advanced_factors,
        sequence_length=60
    )

    features, sentiment, targets = generator.generate()

    # åˆ›å»ºæ•°æ®é›†
    train_size = int(0.8 * len(features))
    train_features = features[:train_size]
    val_features = features[train_size:]
    train_sentiment = sentiment[:train_size]
    val_sentiment = sentiment[train_size:]

    train_targets = {
        'return': targets['return'][:train_size],
        'sharpe': targets['sharpe'][:train_size],
        'drawdown': targets['drawdown'][:train_size]
    }

    val_targets = {
        'return': targets['return'][train_size:],
        'sharpe': targets['sharpe'][train_size:],
        'drawdown': targets['drawdown'][train_size:]
    }

    train_dataset = FactorDataset(
        train_features,
        train_sentiment,
        train_targets,
        sequence_length=60
    )

    val_dataset = FactorDataset(
        val_features,
        val_sentiment,
        val_targets,
        sequence_length=60
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=2
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=2
    )

    # è®­ç»ƒ
    logger.info("Starting training...")
    logger.info(f"Train samples: {len(train_dataset)}")
    logger.info(f"Val samples: {len(val_dataset)}")
    logger.info(f"Batch size: {args.batch_size}")
    logger.info(f"Learning rate: {args.lr}")
    logger.info(f"Epochs: {args.epochs}")
    logger.info(f"Patience: {args.patience}")
    logger.info(f"Device: {args.device}")

    trainer = Trainer(
        config,
        learning_rate=args.lr,
        device=args.device,
        patience=args.patience,
        save_dir=args.save_dir
    )

    trainer.train(
        train_loader,
        val_loader,
        num_epochs=args.epochs,
        save_dir=args.save_dir
    )

    logger.info("Training completed!")


if __name__ == "__main__":
    main()

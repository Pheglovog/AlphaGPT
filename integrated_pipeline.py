"""
AlphaGPT è®­ç»ƒé›†æˆç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ•°æ®éªŒè¯ã€ç¼“å­˜å’Œç›‘æ§æ¨¡å—
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from alphaquant.data_validation import DataValidator, DataCleaner, DataQualityAnalyzer
from alphaquant.data_cache import DataCache
from alphaquant.metrics import TrainingMetrics, PerformanceMonitor, EarlyStoppingMonitor
from train_model_optimized import SyntheticDataGenerator, Trainer, ModelConfig
from alphaquant.model.alpha_quant import AlphaQuant
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from loguru import logger


class AlphaGPTPipeline:
    """AlphaGPT è®­ç»ƒæµæ°´çº¿ï¼ˆé›†æˆç‰ˆï¼‰"""

    def __init__(self):
        # é…ç½®
        self.config = ModelConfig(
            d_model=128,
            nhead=8,
            num_layers=4,
            max_formula_len=64
        )

        # æ•°æ®éªŒè¯å™¨
        self.validator = DataValidator()
        self.cleaner = DataCleaner()
        self.analyzer = DataQualityAnalyzer()

        # æ•°æ®ç¼“å­˜
        self.cache = DataCache()

        # è®­ç»ƒç›‘æ§
        self.metrics = TrainingMetrics()
        self.performance = PerformanceMonitor()
        self.early_stopping = EarlyStoppingMonitor(patience=10, min_delta=1e-6)

    async def run_full_pipeline(
        self,
        num_samples: int = 10000,
        num_epochs: int = 50,
        batch_size: int = 32
    ):
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµæ°´çº¿

        Args:
            num_samples: æ ·æœ¬æ•°é‡
            num_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        logger.info("=" * 60)
        logger.info("AlphaGPT è®­ç»ƒæµæ°´çº¿å¯åŠ¨")
        logger.info("=" * 60)

        # ====== é˜¶æ®µ 1: æ•°æ®ç”Ÿæˆ ======
        logger.info("\n" + "=" * 60)
        logger.info("é˜¶æ®µ 1: æ•°æ®ç”Ÿæˆ")
        logger.info("=" * 60)

        start_time = self.performance.start_timer()

        generator = SyntheticDataGenerator(
            num_samples=num_samples,
            num_factors=self.config.num_basic_factors + self.config.num_advanced_factors,
            sequence_length=60
        )

        features, sentiment, targets = generator.generate()
        logger.info(f"âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼š{num_samples} æ ·æœ¬ï¼Œ{features.shape[1]} å› å­ï¼Œ{features.shape[2]} æ—¶é—´æ­¥")

        # ====== é˜¶æ®µ 2: æ•°æ®éªŒè¯ ======
        logger.info("\n" + "=" * 60)
        logger.info("é˜¶æ®µ 2: æ•°æ®éªŒè¯")
        logger.info("=" * 60)

        # å°† tensor è½¬æ¢ä¸º DataFrameï¼ˆç”¨äºéªŒè¯ï¼‰
        df_samples = []
        for i in range(min(1000, num_samples)):
            sample = {
                'return': targets['return'][i].item(),
                'sharpe': targets['sharpe'][i].item(),
                'drawdown': targets['drawdown'][i].item(),
                'volatility': features[i, :, 0].std().item()  # ç®€åŒ–çš„æ³¢åŠ¨ç‡
            }
            df_samples.append(sample)

        import pandas as pd
        df = pd.DataFrame(df_samples)

        # éªŒè¯æ•°æ®
        is_valid, stats = self.validator.validate_dataframe(df)
        logger.info(self.validator.get_validation_report())

        # ====== é˜¶æ®µ 3: æ•°æ®æ¸…æ´— ======
        logger.info("\n" + "=" * 60)
        logger.info("é˜¶æ®µ 3: æ•°æ®æ¸…æ´—")
        logger.info("=" * 60)

        # ç§»é™¤é‡å¤è¡Œ
        df_clean = self.cleaner.remove_duplicates(df)

        # å¡«å……ç©ºå€¼
        df_clean = self.cleaner.fill_nulls(df_clean, method="ffill")

        # ç§»é™¤å¼‚å¸¸å€¼
        df_clean = self.cleaner.remove_outliers(df_clean, column="return", method="iqr")

        logger.info(self.cleaner.get_cleaning_report())

        # ====== é˜¶æ®µ 4: æ•°æ®è´¨é‡åˆ†æ ======
        logger.info("\n" + "=" * 60)
        logger.info("é˜¶æ®µ 4: æ•°æ®è´¨é‡åˆ†æ")
        logger.info("=" * 60)

        quality_report = self.analyzer.generate_quality_report(df_clean)
        print(quality_report)

        # æ•°æ®åŠ è½½æ—¶é—´
        data_load_time = self.performance.end_timer(start_time, "data_load")

        # ====== é˜¶æ®µ 5: åˆ›å»ºæ•°æ®é›† ======
        logger.info("\n" + "=" * 60)
        logger.info("é˜¶æ®µ 5: åˆ›å»ºæ•°æ®é›†")
        logger.info("=" * 60)

        from torch.utils.data import Dataset, DataLoader

        class OptimizedFactorDataset(Dataset):
            def __init__(self, features, sentiment, targets, seq_len=60):
                self.features = features
                self.sentiment = sentiment
                self.targets = targets
                self.seq_len = seq_len

            def __len__(self):
                return self.features.shape[0] - self.seq_len

            def __getitem__(self, idx):
                feat_seq = self.features[idx:idx+self.seq_len]
                sent = self.sentiment[idx + self.seq_len - 1]

                target_return = self.targets['return'][idx + self.seq_len]
                target_sharpe = self.targets['sharpe'][idx + self.seq_len]
                target_drawdown = self.targets['drawdown'][idx + self.seq_len]

                return {
                    'features': feat_seq,
                    'market_sentiment': sent,
                    'target_return': target_return,
                    'target_sharpe': target_sharpe,
                    'target_drawdown': target_drawdown
                }

        # åˆ†å‰²æ•°æ®é›†
        train_size = int(0.8 * num_samples)

        train_dataset = OptimizedFactorDataset(
            features[:train_size],
            sentiment[:train_size],
            {
                'return': targets['return'][:train_size],
                'sharpe': targets['sharpe'][:train_size],
                'drawdown': targets['drawdown'][:train_size]
            },
            60
        )

        val_dataset = OptimizedFactorDataset(
            features[train_size:],
            sentiment[train_size:],
            {
                'return': targets['return'][train_size:],
                'sharpe': targets['sharpe'][train_size:],
                'drawdown': targets['drawdown'][train_size:]
            },
            60
        )

        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=True
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=2,
            pin_memory=True
        )

        logger.info(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼š{len(train_dataset)} è®­ç»ƒæ ·æœ¬ï¼Œ{len(val_dataset)} éªŒè¯æ ·æœ¬")

        # ====== é˜¶æ®µ 6: è®­ç»ƒ ======
        logger.info("\n" + "=" * 60)
        logger.info("é˜¶æ®µ 6: æ¨¡å‹è®­ç»ƒ")
        logger.info("=" * 60)

        training_start_time = self.performance.start_timer()

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            self.config,
            learning_rate=1e-4,
            weight_decay=1e-5,
            device='cuda',
            patience=10,
            min_delta=1e-6,
            save_dir='./checkpoints'
        )

        # è®­ç»ƒ
        trainer.train(
            train_loader,
            val_loader,
            num_epochs=num_epochs,
            save_dir='./checkpoints'
        )

        training_time = self.performance.end_timer(training_start_time, "training")

        # ====== é˜¶æ®µ 7: ç”ŸæˆæŠ¥å‘Š ======
        logger.info("\n" + "=" * 60)
        logger.info("é˜¶æ®µ 7: ç”ŸæˆæŠ¥å‘Š")
        logger.info("=" * 60)

        # è®­ç»ƒæŒ‡æ ‡
        metrics_report = self.metrics.generate_report()
        print(metrics_report)

        # æ€§èƒ½ç»Ÿè®¡
        perf_stats = self.performance.get_performance_stats()
        logger.info(f"\næ€§èƒ½ç»Ÿè®¡ï¼š")
        logger.info(f"  æ€»è®­ç»ƒæ—¶é—´ï¼š{perf_stats.get('total_training_time', 0):.1f} ç§’")
        logger.info(f"  å¹³å‡ Epoch æ—¶é—´ï¼š{perf_stats.get('avg_epoch_time', 0):.2f} ç§’")
        logger.info(f"  æ€»è¯·æ±‚æ•°ï¼š{self.metrics.metrics_history[-1]['timestamp']} - {self.metrics.metrics_history[0]['timestamp']}")

        # ====== é˜¶æ®µ 8: ç¼“å­˜ç»Ÿè®¡ ======
        logger.info("\n" + "=" * 60)
        logger.info("é˜¶æ®µ 8: ç¼“å­˜ç»Ÿè®¡")
        logger.info("=" * 60)

        cache_stats = self.cache.get_cache_stats()
        cache_report = self.cache.get_cache_report()
        print(cache_report)

        # ====== é˜¶æ®µ 9: æ¸…ç†è¿‡æœŸç¼“å­˜ ======
        logger.info("\n" + "=" * 60)
        logger.info("é˜¶æ®µ 9: æ¸…ç†è¿‡æœŸç¼“å­˜")
        logger.info("=" * 60)

        removed = self.cache.clean_cache(older_than_hours=24)
        logger.info(f"âœ… æ¸…ç†äº† {removed} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")

        logger.info("\n" + "=" * 60)
        logger.info("âœ… å®Œæ•´æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        logger.info("=" * 60)

        # æ€»ç»“
        logger.info("\nğŸ“Š æµæ°´çº¿æ€»ç»“ï¼š")
        logger.info(f"  1. æ•°æ®ç”Ÿæˆï¼š{num_samples} æ ·æœ¬")
        logger.info(f"  2. æ•°æ®éªŒè¯ï¼š{stats['valid_rows']}/{stats['total_rows']} æœ‰æ•ˆ")
        logger.info(f"  3. æ•°æ®æ¸…æ´—ï¼š{self.cleaner.cleaning_stats['rows_cleaned']} è¡Œå¤„ç†")
        logger.info(f"  4. æ•°æ®è´¨é‡ï¼šå®Œæ•´åº¦ {stats.get('total_cells', 0) > 0 and stats['completeness']:.1f}%")
        logger.info(f"  5. æ•°æ®åŠ è½½æ—¶é—´ï¼š{data_load_time:.2f} ç§’")
        logger.info(f"  6. è®­ç»ƒè½®æ•°ï¼š{num_epochs}")
        logger.info(f"  7. æœ€ä½³ Val Lossï¼š{self.metrics.best_metrics['best_val_loss']:.4f} (Epoch {self.metrics.best_metrics['best_epoch']})")
        logger.info(f"  8. ç¼“å­˜å‘½ä¸­ç‡ï¼š{cache_stats['cache_hit_rate']:.2f}%")
        logger.info(f"  9. æ¸…ç†ç¼“å­˜ï¼š{removed} ä¸ªæ–‡ä»¶")

        return {
            'data_generated': num_samples,
            'data_valid': stats['valid_rows'],
            'data_cleaned': self.cleaner.cleaning_stats['rows_cleaned'],
            'training_epochs': num_epochs,
            'best_val_loss': self.metrics.best_metrics['best_val_loss'],
            'best_epoch': self.metrics.best_metrics['best_epoch'],
            'cache_hit_rate': cache_stats['cache_hit_rate']
        }


async def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºæµæ°´çº¿
    pipeline = AlphaGPTPipeline()

    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    results = await pipeline.run_full_pipeline(
        num_samples=10000,
        num_epochs=20,  # ä½¿ç”¨è¾ƒå°‘çš„ epochs è¿›è¡Œæ¼”ç¤º
        batch_size=32
    )

    # æ‰“å°æœ€ç»ˆç»“æœ
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ‰ AlphaGPT è®­ç»ƒæµæ°´çº¿æ¼”ç¤ºå®Œæˆï¼")
    logger.info("=" * 60)
    logger.info("\nä¸»è¦æˆæœï¼š")
    logger.info(f"  âœ… æ•°æ®éªŒè¯å’Œæ¸…æ´—æµç¨‹")
    logger.info(f"  âœ… æ•°æ®ç¼“å­˜ç³»ç»Ÿ")
    logger.info(f"  âœ… è®­ç»ƒç›‘æ§å’Œæ—©åœ")
    logger.info(f"  âœ… æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡")
    logger.info(f"  âœ… å®Œæ•´çš„æµæ°´çº¿é›†æˆ")
    logger.info("\nä¸‹ä¸€æ­¥ï¼š")
    logger.info("  1. ä» Tushare åŠ è½½çœŸå®å¸‚åœºæ•°æ®")
    logger.info("  2. å®ç°æ•°æ®éªŒè¯å’Œæ¸…æ´—çš„è‡ªåŠ¨åŒ–")
    logger.info("  3. ä¼˜åŒ–æ¨¡å‹è¶…å‚æ•°")
    logger.info("  4. è¿›è¡Œæ›´é•¿æ—¶é—´çš„è®­ç»ƒ")
    logger.info("  5. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ")
    logger.info("\n" + "=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
